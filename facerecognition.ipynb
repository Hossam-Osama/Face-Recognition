{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T16:34:21.238458Z",
     "iopub.status.busy": "2025-04-26T16:34:21.237473Z",
     "iopub.status.idle": "2025-04-26T16:34:21.654375Z",
     "shell.execute_reply": "2025-04-26T16:34:21.653732Z",
     "shell.execute_reply.started": "2025-04-26T16:34:21.238415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T16:35:56.515182Z",
     "iopub.status.busy": "2025-04-26T16:35:56.514881Z",
     "iopub.status.idle": "2025-04-26T16:35:58.153732Z",
     "shell.execute_reply": "2025-04-26T16:35:58.152762Z",
     "shell.execute_reply.started": "2025-04-26T16:35:56.515162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "dataset_path = '/kaggle/input/att-database-of-faces'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for subject_id in range(1, 41): \n",
    "    subject_path = os.path.join(dataset_path, f's{subject_id}')\n",
    "    for image_name in sorted(os.listdir(subject_path)):\n",
    "        image_path = os.path.join(subject_path, image_name)\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_flat = img.flatten()\n",
    "        data.append(img_flat)\n",
    "        labels.append(subject_id)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "D = np.array(data)        \n",
    "y = np.array(labels)       \n",
    "\n",
    "print(f'Data matrix D shape: {D.shape}')\n",
    "print(f'Label vector y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T16:36:34.858426Z",
     "iopub.status.busy": "2025-04-26T16:36:34.857727Z",
     "iopub.status.idle": "2025-04-26T16:36:34.868826Z",
     "shell.execute_reply": "2025-04-26T16:36:34.868098Z",
     "shell.execute_reply.started": "2025-04-26T16:36:34.858380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for i in range(0, D.shape[0], 10):\n",
    "    # For each subject, 10 images\n",
    "    train_indices.extend([i, i+2, i+4, i+6, i+8])  # odd indexes\n",
    "    test_indices.extend([i+1, i+3, i+5, i+7, i+9]) # even indexes\n",
    "\n",
    "X_train = D[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = D[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(f'Train set shape: {X_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T16:39:36.382329Z",
     "iopub.status.busy": "2025-04-26T16:39:36.382058Z",
     "iopub.status.idle": "2025-04-26T16:39:36.721093Z",
     "shell.execute_reply": "2025-04-26T16:39:36.720160Z",
     "shell.execute_reply.started": "2025-04-26T16:39:36.382309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_original_faces(X, title=\"Original Faces\"):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2,5,i+1)\n",
    "        plt.imshow(X[i].reshape(112, 92), cmap='gray')  \n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_original_faces(X_train, \"Sample Original Faces (Before PCA)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T16:55:46.174863Z",
     "iopub.status.busy": "2025-04-26T16:55:46.174262Z",
     "iopub.status.idle": "2025-04-26T16:58:44.739955Z",
     "shell.execute_reply": "2025-04-26T16:58:44.739086Z",
     "shell.execute_reply.started": "2025-04-26T16:55:46.174837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PCA_scratch:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.components = None\n",
    "        self.explained_variance_ratio = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # 1. Center the data\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "        \n",
    "        # 2. Compute covariance matrix\n",
    "        covariance_matrix = np.cov(X_centered, rowvar=False)\n",
    "        \n",
    "        # 3. Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "        \n",
    "        # 4. Sort eigenvalues and eigenvectors in descending order\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[sorted_indices]\n",
    "        eigenvectors = eigenvectors[:, sorted_indices]\n",
    "        \n",
    "        self.components = eigenvectors\n",
    "        total_variance = np.sum(eigenvalues)\n",
    "        self.explained_variance_ratio = eigenvalues / total_variance\n",
    "\n",
    "    def transform(self, X, variance_threshold=0.9):\n",
    "        X_centered = X - self.mean\n",
    "        cumulative_variance = np.cumsum(self.explained_variance_ratio)\n",
    "        n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n",
    "        W = self.components[:, :n_components]  # Take top eigenvectors\n",
    "        return X_centered @ W, n_components\n",
    "\n",
    "pca = PCA_scratch()\n",
    "pca.fit(X_train)\n",
    "\n",
    "\n",
    "alphas = [0.8, 0.85, 0.9, 0.95]\n",
    "X_train_pca = {}\n",
    "\n",
    "for alpha in alphas:\n",
    "    X_reduced, n_components = pca.transform(X_train, variance_threshold=alpha)\n",
    "    X_train_pca[alpha] = X_reduced\n",
    "    print(f'Alpha {alpha}: reduced dimensionality = {n_components} components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T17:01:32.751148Z",
     "iopub.status.busy": "2025-04-26T17:01:32.750764Z",
     "iopub.status.idle": "2025-04-26T17:01:33.026190Z",
     "shell.execute_reply": "2025-04-26T17:01:33.025188Z",
     "shell.execute_reply.started": "2025-04-26T17:01:32.751123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_sample_faces(X_pca, title):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2,5,i+1)\n",
    "        plt.imshow(X_pca[i].reshape(1, -1), cmap='gray', aspect='auto')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example for alpha = 0.9\n",
    "plot_sample_faces(X_train_pca[0.9], \"Faces in PCA Space (alpha=0.9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T16:40:42.409534Z",
     "iopub.status.busy": "2025-04-26T16:40:42.408652Z",
     "iopub.status.idle": "2025-04-26T16:40:47.167761Z",
     "shell.execute_reply": "2025-04-26T16:40:47.166939Z",
     "shell.execute_reply.started": "2025-04-26T16:40:42.409506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# def apply_pca(X, variance_threshold=0.9):\n",
    "#     pca = PCA()\n",
    "#     pca.fit(X)\n",
    "    \n",
    "#     # Cumulative explained variance\n",
    "#     cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "#     # Find number of components to retain the given variance\n",
    "#     n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n",
    "    \n",
    "#     # Apply PCA with the found number of components\n",
    "#     pca = PCA(n_components=n_components)\n",
    "#     X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "#     return X_reduced, pca\n",
    "\n",
    "# # Try for alpha = 0.8\n",
    "# alphas = [0.8, 0.85, 0.9, 0.95]\n",
    "# pca_models = {}\n",
    "# X_train_pca = {}\n",
    "\n",
    "# for alpha in alphas:\n",
    "#     X_reduced, pca_model = apply_pca(X_train, variance_threshold=alpha)\n",
    "#     pca_models[alpha] = pca_model\n",
    "#     X_train_pca[alpha] = X_reduced\n",
    "#     print(f'Alpha {alpha}: reduced dimensionality = {X_reduced.shape[1]} components')\n",
    "\n",
    "# # --- Optional: Visualize a few images in PCA space ---\n",
    "# def plot_sample_faces(X_pca, title):\n",
    "#     plt.figure(figsize=(10,5))\n",
    "#     for i in range(10):\n",
    "#         plt.subplot(2,5,i+1)\n",
    "#         plt.imshow(X_pca[i].reshape(1,-1), cmap='gray', aspect='auto')\n",
    "#         plt.axis('off')\n",
    "#     plt.suptitle(title)\n",
    "#     plt.show()\n",
    "\n",
    "# # Example for alpha = 0.9\n",
    "# plot_sample_faces(X_train_pca[0.9], \"Faces in PCA Space (alpha=0.9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans_scratch:\n",
    "    def __init__(self, n_clusters=10, max_iter=300, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        np.random.seed(self.random_state)\n",
    "        # Randomly initialize centroids\n",
    "        random_indices = np.random.choice(len(X), self.n_clusters, replace=False)\n",
    "        self.centroids = X[random_indices]\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "\n",
    "            # Compute new centroids\n",
    "            new_centroids = np.array([X[labels == i].mean(axis=0) if np.any(labels == i) else self.centroids[i] \n",
    "                                      for i in range(self.n_clusters)])\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.allclose(self.centroids, new_centroids):\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "        self.labels_ = labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [20, 40, 60]\n",
    "kmeans_results = {}\n",
    "\n",
    "for alpha in alphas:  \n",
    "    X_reduced = X_train_pca[alpha]\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans_scratch(n_clusters=k)\n",
    "        kmeans.fit(X_reduced)\n",
    "        labels = kmeans.labels_\n",
    "        kmeans_results[(alpha, k)] = labels\n",
    "        print(f'KMeans done for alpha={alpha}, K={k}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def clustering_accuracy(y_true, y_pred):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Find the best matching between clusters and true labels\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "    accuracy = cm[row_ind, col_ind].sum() / np.sum(cm)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "\n",
    "for (alpha, k), labels in kmeans_results.items():\n",
    "    acc = clustering_accuracy(y_train, labels)  # assuming y_train is your label vector\n",
    "    accuracies[(alpha, k)] = acc\n",
    "    print(f'Accuracy for alpha={alpha}, K={k}: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy vs K (for each alpha)\n",
    "for alpha in alphas:\n",
    "    acc = [accuracies[(alpha, k)] for k in k_values]\n",
    "    plt.plot(k_values, acc, label=f'Alpha={alpha}')\n",
    "\n",
    "plt.xlabel('K (number of clusters)')\n",
    "plt.ylabel('Clustering Accuracy')\n",
    "plt.title('Clustering Accuracy vs K for different Alphas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy vs alpha (for each K)\n",
    "for k in k_values:\n",
    "    acc = [accuracies[(alpha, k)] for alpha in alphas]\n",
    "    plt.plot(alphas, acc, label=f'K={k}')\n",
    "\n",
    "plt.xlabel('Alpha (variance threshold)')\n",
    "plt.ylabel('Clustering Accuracy')\n",
    "plt.title('Clustering Accuracy vs Alpha for different Ks')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus not finished yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T17:03:33.873560Z",
     "iopub.status.busy": "2025-04-26T17:03:33.873135Z",
     "iopub.status.idle": "2025-04-26T17:03:38.208469Z",
     "shell.execute_reply": "2025-04-26T17:03:38.207539Z",
     "shell.execute_reply.started": "2025-04-26T17:03:33.873532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=10304, bottleneck_dim=50):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, bottleneck_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim),\n",
    "            nn.Sigmoid()  # output in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T17:03:41.522172Z",
     "iopub.status.busy": "2025-04-26T17:03:41.521724Z",
     "iopub.status.idle": "2025-04-26T17:03:41.575616Z",
     "shell.execute_reply": "2025-04-26T17:03:41.574889Z",
     "shell.execute_reply.started": "2025-04-26T17:03:41.522148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0,1]\n",
    "X_train_norm = X_train / 255.0\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train_tensor = X_train_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T17:03:44.014174Z",
     "iopub.status.busy": "2025-04-26T17:03:44.013842Z",
     "iopub.status.idle": "2025-04-26T17:05:30.764734Z",
     "shell.execute_reply": "2025-04-26T17:05:30.763777Z",
     "shell.execute_reply.started": "2025-04-26T17:03:44.014152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "autoencoder = Autoencoder(input_dim=10304, bottleneck_dim=50).to(device)\n",
    "\n",
    "# Optimizer and Loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "print(\"Training Autoencoder...\")\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(X_train_tensor.size()[0])\n",
    "    \n",
    "    for i in range(0, X_train_tensor.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch = X_train_tensor[indices]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T17:05:30.766542Z",
     "iopub.status.busy": "2025-04-26T17:05:30.766095Z",
     "iopub.status.idle": "2025-04-26T17:05:30.810397Z",
     "shell.execute_reply": "2025-04-26T17:05:30.809497Z",
     "shell.execute_reply.started": "2025-04-26T17:05:30.766521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "autoencoder.eval()\n",
    "\n",
    "# Extract bottleneck features\n",
    "with torch.no_grad():\n",
    "    features = autoencoder.encoder(X_train_tensor).cpu().numpy()\n",
    "\n",
    "print(f\"Bottleneck features shape: {features.shape}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T17:05:50.918844Z",
     "iopub.status.busy": "2025-04-26T17:05:50.918458Z",
     "iopub.status.idle": "2025-04-26T17:05:51.672107Z",
     "shell.execute_reply": "2025-04-26T17:05:51.671355Z",
     "shell.execute_reply.started": "2025-04-26T17:05:50.918814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=40, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(features)\n",
    "\n",
    "# Apply GMM\n",
    "gmm = GaussianMixture(n_components=40, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(features)\n",
    "\n",
    "\n",
    "\n",
    "print(\"K-Means clustering done.\")\n",
    "print(\"GMM clustering done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T17:06:07.714304Z",
     "iopub.status.busy": "2025-04-26T17:06:07.713983Z",
     "iopub.status.idle": "2025-04-26T17:06:08.467916Z",
     "shell.execute_reply": "2025-04-26T17:06:08.466910Z",
     "shell.execute_reply.started": "2025-04-26T17:06:07.714275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reconstruct some images\n",
    "with torch.no_grad():\n",
    "    reconstructed = autoencoder(X_train_tensor).cpu().numpy()\n",
    "\n",
    "def plot_reconstructed_faces(originals, reconstructed, title=\"Reconstructed Faces\"):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for i in range(10):\n",
    "        # Original\n",
    "        plt.subplot(2, 10, i+1)\n",
    "        plt.imshow(originals[i].reshape(112,92), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Reconstructed\n",
    "        plt.subplot(2, 10, i+11)\n",
    "        plt.imshow(reconstructed[i].reshape(112,92), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_reconstructed_faces(X_train_norm, reconstructed, title=\"Original vs Reconstructed Faces (Autoencoder)\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 244146,
     "sourceId": 847361,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
